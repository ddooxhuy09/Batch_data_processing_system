{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window as W\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as sf\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "spark = SparkSession.builder.config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.3.0')\\\n",
    "                            .config(\"spark.jars.packages\",\"com.mysql:mysql-connector-j:8.0.31\")\\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_clicks(df):\n",
    "    clicks_data = df.filter(df.custom_track == 'click')\n",
    "    clicks_data = clicks_data.na.fill({'bid': 0})\n",
    "    clicks_data = clicks_data.na.fill({'job_id': 0})\n",
    "    clicks_data = clicks_data.na.fill({'publisher_id': 0})\n",
    "    clicks_data = clicks_data.na.fill({'group_id': 0})\n",
    "    clicks_data = clicks_data.na.fill({'campaign_id': 0})\n",
    "    clicks_data.registerTempTable('clicks')\n",
    "    clicks_output = spark.sql(\"\"\"select job_id , date(ts) as date , hour(ts) as hour , publisher_id , campaign_id , group_id , ROUND(avg(bid),1) as bid_set, count(*) as clicks , sum(bid) as spend_hour from clicks\n",
    "    group by job_id , date(ts) , hour(ts) , publisher_id , campaign_id , group_id \"\"\")\n",
    "    return clicks_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_conversion(df):\n",
    "    conversion_data = df.filter(df.custom_track == 'conversion')\n",
    "    conversion_data = conversion_data.na.fill({'job_id':0})\n",
    "    conversion_data = conversion_data.na.fill({'publisher_id':0})\n",
    "    conversion_data = conversion_data.na.fill({'group_id':0})\n",
    "    conversion_data = conversion_data.na.fill({'campaign_id':0})\n",
    "    conversion_data.registerTempTable('conversion')\n",
    "    conversion_output = spark.sql(\"\"\"select job_id , date(ts) as date , hour(ts) as hour , publisher_id , campaign_id , group_id , count(*) as conversions  from conversion\n",
    "    group by job_id , date(ts) , hour(ts) , publisher_id , campaign_id , group_id \"\"\")\n",
    "    return conversion_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_qualified(df):    \n",
    "    qualified_data = df.filter(df.custom_track == 'qualified')\n",
    "    qualified_data = qualified_data.na.fill({'job_id':0})\n",
    "    qualified_data = qualified_data.na.fill({'publisher_id':0})\n",
    "    qualified_data = qualified_data.na.fill({'group_id':0})\n",
    "    qualified_data = qualified_data.na.fill({'campaign_id':0})\n",
    "    qualified_data.registerTempTable('qualified')\n",
    "    qualified_output = spark.sql(\"\"\"select job_id , date(ts) as date , hour(ts) as hour , publisher_id , campaign_id , group_id , count(*) as qualified  from qualified\n",
    "    group by job_id , date(ts) , hour(ts) , publisher_id , campaign_id , group_id \"\"\")\n",
    "    return qualified_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_unqualified(df):\n",
    "    unqualified_data = df.filter(df.custom_track == 'unqualified')\n",
    "    unqualified_data = unqualified_data.na.fill({'job_id':0})\n",
    "    unqualified_data = unqualified_data.na.fill({'publisher_id':0})\n",
    "    unqualified_data = unqualified_data.na.fill({'group_id':0})\n",
    "    unqualified_data = unqualified_data.na.fill({'campaign_id':0})\n",
    "    unqualified_data.registerTempTable('unqualified')\n",
    "    unqualified_output = spark.sql(\"\"\"select job_id , date(ts) as date , hour(ts) as hour , publisher_id , campaign_id , group_id , count(*) as unqualified  from unqualified\n",
    "    group by job_id , date(ts) , hour(ts) , publisher_id , campaign_id , group_id \"\"\")\n",
    "    return unqualified_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_final_data(clicks_output,conversion_output,qualified_output,unqualified_output):\n",
    "    final_data = clicks_output.join(conversion_output,['job_id','date','hour','publisher_id','campaign_id','group_id'],'full').\\\n",
    "    join(qualified_output,['job_id','date','hour','publisher_id','campaign_id','group_id'],'full').\\\n",
    "    join(unqualified_output,['job_id','date','hour','publisher_id','campaign_id','group_id'],'full')\n",
    "    return final_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cassandra_data(df):\n",
    "    clicks_output = calculating_clicks(df)\n",
    "    conversion_output = calculating_conversion(df)\n",
    "    qualified_output = calculating_qualified(df)\n",
    "    unqualified_output = calculating_unqualified(df)\n",
    "    final_data = process_final_data(clicks_output,conversion_output,qualified_output,unqualified_output)\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_company_data(url, driver, dbtable_company, user, password):\n",
    "    company = spark.read.format('jdbc').options(url=url, driver=driver, dbtable=dbtable_company, user=user, password=password).load()\n",
    "    company = company.select(\"id\", \"company_id\", \"group_id\",\"campaign_id\").withColumnRenamed(\"id\", \"job_id\")\n",
    "    return company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_to_mysql(output,url,driver,dbtable,user,password):\n",
    "    final_output = output.select('job_id', 'date', 'hour', 'publisher_id', 'company_id', 'campaign_id','group_id', \n",
    "                                 'unqualified', 'qualified', 'conversions', 'clicks', 'bid_set', 'spend_hour')\n",
    "    final_output = final_output.withColumnRenamed('date', 'dates')\\\n",
    "                               .withColumnRenamed('hour', 'hours')\\\n",
    "                               .withColumnRenamed('qualified', 'qualified_application')\\\n",
    "                               .withColumnRenamed('unqualified', 'disqualified_application')\\\n",
    "                               .withColumnRenamed('conversions', 'conversion')\n",
    "\n",
    "    final_output = final_output.withColumn('sources', lit('Cassandra'))\n",
    "    final_output = final_output.withColumn(\"latest_update_time\", current_timestamp())\n",
    "    final_output.printSchema()\n",
    "    final_output.write.format(\"jdbc\") \\\n",
    "                      .option(\"url\", url) \\\n",
    "                      .option(\"driver\", driver) \\\n",
    "                      .option(\"dbtable\", dbtable) \\\n",
    "                      .option(\"user\", user) \\\n",
    "                      .option(\"password\", password) \\\n",
    "                      .mode(\"append\") \\\n",
    "                      .save()\n",
    "    return print('Data imported successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_task(mysql_time, host, port, db_name, url, driver, dbtable, user, password):\n",
    "    print('The host is ', host)\n",
    "    print('The port using is ', port)\n",
    "    print('The db using is ', db_name)\n",
    "    print('-----------------------------')\n",
    "    print('Retrieving data from Cassandra')\n",
    "    print('-----------------------------')\n",
    "    df = spark.read.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "                   .options(table=\"tracking\", keyspace=\"study_de\")\\\n",
    "                   .load()\\\n",
    "                   .where(col('ts') >= mysql_time)\n",
    "    print('-----------------------------')\n",
    "    print('Selecting data from Cassandra')\n",
    "    print('-----------------------------')\n",
    "    df = df.select('ts', 'job_id', 'custom_track', 'bid','campaign_id', 'group_id', 'publisher_id')\n",
    "    df = df.filter(df.job_id.isNotNull())\n",
    "    df.printSchema()\n",
    "    print('-----------------------------')\n",
    "    print('Processing Cassandra Output')\n",
    "    print('-----------------------------')\n",
    "    cassandra_output = process_cassandra_data(df)\n",
    "    print('-----------------------------')\n",
    "    print('Merge Company Data')\n",
    "    print('-----------------------------')\n",
    "    dbtable_company = \"job\"\n",
    "    company = retrieve_company_data(url, driver, dbtable_company, user, password)\n",
    "    print('-----------------------------')\n",
    "    print('Finalizing Output')\n",
    "    print('-----------------------------')\n",
    "    final_output = cassandra_output.join(company, 'job_id', 'left').drop(company.group_id).drop(company.campaign_id)\n",
    "    print('-----------------------------')\n",
    "    print('Import Output to MySQL')\n",
    "    print('-----------------------------')\n",
    "    import_to_mysql(final_output, url, driver, dbtable, user, password)\n",
    "    return print('Task Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_time_cassandra():\n",
    "    data = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table='tracking', keyspace='study_de').load()\n",
    "    cassandra_latest_time = data.agg({'ts': 'max'}).take(1)[0][0]\n",
    "    return cassandra_latest_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mysql_latest_time(url, driver,dbtable, user, password):\n",
    "    mysql_time = spark.read.format('jdbc')\\\n",
    "                           .option('url',url)\\\n",
    "                           .option('driver',driver)\\\n",
    "                           .option('dbtable',dbtable)\\\n",
    "                           .option('user',user)\\\n",
    "                           .option('password',password)\\\n",
    "                           .load()\n",
    "    mysql_time = mysql_time.agg({'latest_update_time': 'max'}).take(1)[0][0]\n",
    "    if mysql_time is None:\n",
    "        mysql_latest = '1998-01-01 23:59:59'\n",
    "    else:\n",
    "        mysql_latest = mysql_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return mysql_latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'localhost'\n",
    "port = '3306'\n",
    "db_name = 'data_engineer'\n",
    "user = 'root'\n",
    "password = 'mysql'\n",
    "url = 'jdbc:mysql://' + host + ':' + port + '/' + db_name\n",
    "driver = \"com.mysql.cj.jdbc.Driver\"\n",
    "dbtable = \"result\"\n",
    "while True:\n",
    "    start_time = datetime.datetime.now()\n",
    "    cassandra_time = get_latest_time_cassandra()\n",
    "    print('Cassandra latest time is {}'.format(cassandra_time))\n",
    "    mysql_time = get_mysql_latest_time(url, driver, dbtable, user, password)\n",
    "    print('MySQL latest time is'.format(mysql_time))\n",
    "    if cassandra_time > mysql_time:\n",
    "        main_task(mysql_time, host, port, db_name, url, driver, dbtable, user, password)\n",
    "    else:\n",
    "        print(\"No new data found\")\n",
    "    end_time = datetime.datetime.now()\n",
    "    execution_time = (end_time - start_time).total_seconds()\n",
    "    print('Job takes {} seconds to execute'.format(execution_time))\n",
    "    time.sleep(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
